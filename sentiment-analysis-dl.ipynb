{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Solution for Sentiment Analysis\n",
    "\n",
    "    \n",
    "#### LSTM (Long short term memory)\n",
    "* test dataset: \n",
    "  * took > 1 hour\n",
    "  * train : test = 2 :1\n",
    "  * tried activation functions:\n",
    "      * tanh function: doesn't work\n",
    "      * relu function: doesn't work, values get too large to `nan`\n",
    "      * sigmoid function: better for binary classification\n",
    "      * softmax function: works well for multiple classification\n",
    "  * acc (softmax): 71% \n",
    "  * label accuracy =  0.7170868347338936\n",
    "\n",
    "* whole dataset:\n",
    "  * train : test = 9 :1\n",
    "  * Trainning model  LSTM_v0.1.h5  successfully, took(s):  66079.6750668 =18.4 hours\n",
    "  * Evaluating model took(s):  197.92229590000352,  acc: 0.79\n",
    "  * label accuracy =  0.7927490188749766\n",
    "  \n",
    "TODO/Imporvements:\n",
    "* preprocess the reviews:\n",
    "  * TF*IDF to reduce common words\n",
    "* represent the reviews:\n",
    "  * word2vec\n",
    "  * use sementic knowledgebase: freebase, freebase, satori, wordnet, etc\n",
    "  \n",
    "* try diff superparams\n",
    "* try BRNN, GRU-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (107018, 1148) X_train.shape (96316, 1148) X_test.shape (10702, 1148)\n",
      "Y.shape (107018, 5) Y_train.shape (96316, 5) Y_test.shape (10702, 5)\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n",
    "def create_model(max_fatures, embed_dim, input_length, lstm_out):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_fatures, embed_dim,input_length = input_length))\n",
    "    model.add(SpatialDropout1D(0.4))\n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(5,activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "    \n",
    "def preprocess_data(dataFile, max_fatures):\n",
    "    data = pd.read_csv(dataFile)\n",
    "    data = data[['Review','Label']]\n",
    "\n",
    "    data['Review'] = data['Review'].apply(lambda x: x.lower())\n",
    "    data['Review'] = data['Review'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "    tokenizer.fit_on_texts(data['Review'].values)\n",
    "    X = tokenizer.texts_to_sequences(data['Review'].values)\n",
    "    X = pad_sequences(X)\n",
    "    Y = pd.get_dummies(data['Label']).values\n",
    "    \n",
    "    return X, Y\n",
    "    \n",
    "def train_model(X_train, Y_train, batch_size, modelName):\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)\n",
    "    model.save(modelName) \n",
    "    stop = timeit.default_timer()\n",
    "    print('Trainning model ', modelName, ' successfully, took(s): ', stop - start)\n",
    "\n",
    "# super params\n",
    "max_fatures = 2000\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "batch_size = 32\n",
    "\n",
    "X,Y = preprocess_data('../100k-courseras-course-reviews-dataset/reviews.csv', max_fatures)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "print('X.shape', X.shape, 'X_train.shape', X_train.shape, 'X_test.shape', X_test.shape)\n",
    "print('Y.shape', Y.shape, 'Y_train.shape', Y_train.shape, 'Y_test.shape', Y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1148, 128)         256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 1148, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 985       \n",
      "=================================================================\n",
      "Total params: 511,785\n",
      "Trainable params: 511,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/7\n",
      " - 9401s - loss: 0.6377 - acc: 0.7661\n",
      "Epoch 2/7\n",
      " - 9283s - loss: 0.5565 - acc: 0.7877\n",
      "Epoch 3/7\n",
      " - 9334s - loss: 0.5376 - acc: 0.7942\n",
      "Epoch 4/7\n",
      " - 9364s - loss: 0.5240 - acc: 0.7998\n",
      "Epoch 5/7\n",
      " - 9394s - loss: 0.5121 - acc: 0.8041\n",
      "Epoch 6/7\n",
      " - 9443s - loss: 0.5013 - acc: 0.8090\n",
      "Epoch 7/7\n",
      " - 9859s - loss: 0.4904 - acc: 0.8123\n",
      "Trainning model  LSTM_v0.1.h5  successfully, took(s):  66079.6750668\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "\n",
    "modelName = 'LSTM_v0.1.h5'\n",
    "\n",
    "input_length = X_train.shape[1]\n",
    "model =  create_model(max_fatures, embed_dim, input_length, lstm_out)\n",
    "train_model(X_train, Y_train, batch_size, modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "from keras.models import load_model\n",
    "\n",
    "def evaluate_model(modelName, X_test, Y_test, batch_size):\n",
    "    model = load_model(modelName)\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "    stop = timeit.default_timer()\n",
    "    print('Evaluating model took(s): ', stop - start)\n",
    "\n",
    "    print(\"score: %.2f\" % (score))\n",
    "    print(\"acc: %.2f\" % (acc))\n",
    "    \n",
    "    results = model.predict(X_test, batch_size=batch_size, verbose=2, steps=None)\n",
    "    predict_labels = [np.argmax(x)+1 for x in results]\n",
    "    actual_labels = [np.argmax(x)+1 for x in Y_test]\n",
    "    corrects = [ 1 if predict_labels[i] == actual_labels[i] else 0 for i in range(0, len(actual_labels))]\n",
    "    acc = sum(corrects)/len(corrects)\n",
    "    print('label accuracy = ', acc)\n",
    "    \n",
    "evaluate_model(modelName, X_test, Y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (10702, 733) X_train.shape (9631, 733) X_test.shape (1071, 733)\n",
      "Y.shape (10702, 5) Y_train.shape (9631, 5) Y_test.shape (1071, 5)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 733, 128)          256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_9 (Spatial (None, 733, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 985       \n",
      "=================================================================\n",
      "Total params: 511,785\n",
      "Trainable params: 511,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/7\n",
      " - 623s - loss: 0.8756 - acc: 0.6956\n",
      "Epoch 2/7\n",
      " - 605s - loss: 0.7200 - acc: 0.7269\n",
      "Epoch 3/7\n",
      " - 611s - loss: 0.6634 - acc: 0.7398\n",
      "Epoch 4/7\n",
      " - 606s - loss: 0.6171 - acc: 0.7574\n",
      "Epoch 5/7\n",
      " - 609s - loss: 0.5813 - acc: 0.7776\n",
      "Epoch 6/7\n",
      " - 606s - loss: 0.5436 - acc: 0.7892\n",
      "Epoch 7/7\n",
      " - 614s - loss: 0.5176 - acc: 0.7998\n",
      "Trainning model  LSTM_test_softmax_v0.1.h5  successfully, took(s):  4276.7561859000125\n",
      "Evaluating model took(s):  17.52764020000177\n",
      "score: 0.77\n",
      "acc: 0.72\n",
      "label accuracy =  0.7170868347338936\n"
     ]
    }
   ],
   "source": [
    "### Play with more Experiments ####\n",
    "\n",
    "def create_model(max_fatures, embed_dim, input_length, lstm_out):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_fatures, embed_dim,input_length = input_length))\n",
    "    model.add(SpatialDropout1D(0.4))\n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(5,activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# super params\n",
    "max_fatures = 2000\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "batch_size = 32\n",
    "\n",
    "X,Y = preprocess_data('../100k-courseras-course-reviews-dataset/reviews_test.csv', max_fatures)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "print('X.shape', X.shape, 'X_train.shape', X_train.shape, 'X_test.shape', X_test.shape)\n",
    "print('Y.shape', Y.shape, 'Y_train.shape', Y_train.shape, 'Y_test.shape', Y_test.shape)\n",
    "\n",
    "modelName = 'LSTM_test_softmax_v0.1.h5'\n",
    "\n",
    "input_length = X_train.shape[1]\n",
    "model =  create_model(max_fatures, embed_dim, input_length, lstm_out)\n",
    "train_model(X_train, Y_train, batch_size, modelName)\n",
    "\n",
    "evaluate_model(modelName, X_test, Y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
