{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader Sentiment\n",
    "\n",
    "* compound: [-1, 1]\n",
    "  * positive sentiment: compound score >= 0.05\n",
    "    * [0.05, 1] -> 4, 5\n",
    "    * [0.05, 0.525] ->4\n",
    "    * [0.525, 1] -> 5\n",
    "  * neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    * (-0.05, 0.05) -> 3\n",
    "  * negative sentiment: compound score <= -0.05\n",
    "    * [-1, -0.05] -> 1, 2\n",
    "    * [-1, -0.525] -> 1\n",
    "    * [-0.525, -0.05] -> 2\n",
    "\n",
    "### TextBlob\n",
    "\n",
    "* polarity: [-1, 1]\n",
    "* subjectivity: [0, 1]\n",
    "    \n",
    "### Stanford CoreNLP\n",
    "* use deeplearning algorithm to handle the order of words\n",
    "* sentimentValue= 4 , sentiment= Verypositive\n",
    "* sentimentValue= 3 , sentiment= Positive\n",
    "* sentimentValue= 2 , sentiment= Neutral\n",
    "* sentimentValue= 1 , sentiment= Negative\n",
    "* sentimentValue= 0 , sentiment= Verynegative\n",
    "    \n",
    "### LSTM\n",
    "* test dataset: \n",
    "  * took > 1 hour\n",
    "  * train : test = 2 :1\n",
    "  * acc: 71% of 1500 validation size on testing dataset, \n",
    "* train dataset:  \n",
    "  * took 44725.2392093s =12.4 hours\n",
    "  * train : test = 2 :1\n",
    "  * acc: 79% on whole test dataset by model.evaluate()\n",
    "  * acc: 100% on whole test dataset by labels compare\n",
    "    * predict 31785 samples, took 1002s,  31.5ms/sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"It's not a course... this is a very short general introduction to 3D Printing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not a course... this is a very short general introduction to 3D Printing {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "score = analyser.polarity_scores(sentence)\n",
    "print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "score['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02500000000000001\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "testimonial = TextBlob(sentence)\n",
    "print(testimonial.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Negative\n",
      "[0.13210145953291, 0.65314998335187, 0.19261037575247, 0.01472903402969, 0.00740914733306]\n",
      "(ROOT|sentiment=1|prob=0.653\r\n",
      "  (@S|sentiment=2|prob=0.590\r\n",
      "    (S|sentiment=2|prob=0.509 (NP|sentiment=2|prob=0.996 It)\r\n",
      "      (VP|sentiment=2|prob=0.583\r\n",
      "        (@VP|sentiment=2|prob=0.857 (VBZ|sentiment=2|prob=0.994 's) (RB|sentiment=1|prob=0.974 not))\r\n",
      "        (NP|sentiment=2|prob=0.964 (DT|sentiment=2|prob=0.990 a) (NN|sentiment=2|prob=0.993 course))))\r\n",
      "    (:|sentiment=2|prob=0.997 ...))\r\n",
      "  (S|sentiment=1|prob=0.461 (NP|sentiment=2|prob=0.998 this)\r\n",
      "    (VP|sentiment=1|prob=0.439 (VBZ|sentiment=2|prob=0.989 is)\r\n",
      "      (NP|sentiment=2|prob=0.618\r\n",
      "        (NP|sentiment=2|prob=0.456 (DT|sentiment=2|prob=0.990 a)\r\n",
      "          (@NP|sentiment=2|prob=0.569\r\n",
      "            (ADJP|sentiment=2|prob=0.706 (RB|sentiment=2|prob=0.995 very) (JJ|sentiment=2|prob=0.988 short))\r\n",
      "            (@NP|sentiment=2|prob=0.961 (JJ|sentiment=2|prob=0.993 general) (NN|sentiment=2|prob=0.990 introduction))))\r\n",
      "        (PP|sentiment=2|prob=0.921 (TO|sentiment=2|prob=0.990 to)\r\n",
      "          (NP|sentiment=2|prob=0.895 (NN|sentiment=2|prob=0.966 3D) (NN|sentiment=2|prob=0.631 Printing)))))))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "output = nlp.annotate(sentence, properties={\n",
    "            'annotators': 'sentiment',\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "print(output['sentences'][0]['sentimentValue'])\n",
    "print(output['sentences'][0]['sentiment'])\n",
    "print(output['sentences'][0]['sentimentDistribution'])\n",
    "print(output['sentences'][0]['sentimentTree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02500000000000001\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import sentiment\n",
    "\n",
    "scores = sentiment(sentence)\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deep Learning Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4244\n",
      "3844\n",
      "8854\n",
      "32156\n",
      "143534\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('../100k-courseras-course-reviews-dataset/reviews_train.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['Review','Label']]\n",
    "\n",
    "data['Review'] = data['Review'].apply(lambda x: x.lower())\n",
    "data['Review'] = data['Review'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "print(data[ data['Label'] == 1].size)\n",
    "print(data[ data['Label'] == 2].size)\n",
    "print(data[ data['Label'] == 3].size)\n",
    "print(data[ data['Label'] == 4].size)\n",
    "print(data[ data['Label'] == 5].size)\n",
    "\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['Review'].values)\n",
    "X = tokenizer.texts_to_sequences(data['Review'].values)\n",
    "X = pad_sequences(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1148, 128)         256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 1148, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 985       \n",
      "=================================================================\n",
      "Total params: 511,785\n",
      "Trainable params: 511,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64531, 1148) (64531, 5)\n",
      "(31785, 1148) (31785, 5)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['Label']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " - 6236s - loss: 0.6490 - acc: 0.7657\n",
      "Epoch 2/7\n",
      " - 6282s - loss: 0.5629 - acc: 0.7872\n",
      "Epoch 3/7\n",
      " - 6296s - loss: 0.5346 - acc: 0.7980\n",
      "Epoch 4/7\n",
      " - 6336s - loss: 0.5202 - acc: 0.8042\n",
      "Epoch 5/7\n",
      " - 6297s - loss: 0.5055 - acc: 0.8081\n",
      "Epoch 6/7\n",
      " - 6521s - loss: 0.4920 - acc: 0.8148\n",
      "Epoch 7/7\n",
      " - 6757s - loss: 0.4793 - acc: 0.8188\n",
      "Time:  44725.2392093\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)\n",
    "\n",
    "model.save('LSTM_train_v0.1.h5') \n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31785, 1148) (31785, 5)\n",
      "score: 0.57\n",
      "acc: 0.79\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "print(X_test.shape,Y_test.shape)\n",
    "model = load_model('LSTM_train_v0.1.h5')\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lstm model on test data set, got score: 0.80, acc: 0.71\n",
    "# model.save('LSTM_test_v0.1.h5') \n",
    "# first lstm model on train data set, got score: 0.57, acc: 0.79\n",
    "# model.save('LSTM_train_v0.1.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  1.0\n",
      "Predicting tooks(s):  1002.3016105999995\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "model = load_model('LSTM_train_v0.1.h5')\n",
    "results = model.predict(X_test, batch_size=batch_size, verbose=2, steps=None)\n",
    "predict_labels = [np.argmax(x)+1 for x in results]\n",
    "actual_labels = [np.argmax(x)+1 for x in Y_test]\n",
    "corrects = [ 1 if predict_labels[i] == actual_labels[i] else 0 for i in range(0, len(labels))]\n",
    "acc = sum(corrects)/len(corrects)\n",
    "print('label accuracy = ', acc)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Predicting tooks(s): ', stop - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "labels = [np.argmax(x)+1 for x in results]\n",
    "actual_labels = [np.argmax(x)+1 for x in Y_test[0:5]]\n",
    "corrects = [ 1 if labels[i] == actual_labels[i] else 0 for i in range(0, len(labels))]\n",
    "print(sum(corrects)/len(corrects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 good and interesting\n",
      "1    this class is very helpful to me currently im ...\n",
      "2    likeprof and tas are helpful and the discussio...\n",
      "Name: Review, dtype: object\n",
      "(107018, 1148)\n",
      "[[   0    0    0 ...   16    2   39]\n",
      " [   0    0    0 ...  102  460   78]\n",
      " [   0    0    0 ... 1045   31  115]]\n"
     ]
    }
   ],
   "source": [
    "############# preprocess for LSTM\n",
    "\n",
    "def get_tokenizer(dataFile, max_fatures):\n",
    "    data = pd.read_csv(dataFile)\n",
    "    data = data[['Review','Label']]\n",
    "\n",
    "    data['Review'] = data['Review'].apply(lambda x: x.lower())\n",
    "    data['Review'] = data['Review'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "    tokenizer.fit_on_texts(data['Review'].values)\n",
    "    X = tokenizer.texts_to_sequences(data['Review'].values)\n",
    "    X = pad_sequences(X)\n",
    "    print(data['Review'][0:3])\n",
    "    print(X.shape)\n",
    "    print(X[0:3])\n",
    "    Y = pd.get_dummies(data['Label']).values\n",
    "    \n",
    "    return tokenizer, X.shape[1]\n",
    "\n",
    "tokenizer, feature_len = get_tokenizer('../100k-courseras-course-reviews-dataset/reviews.csv', max_fatures = 2000)\n",
    "\n",
    "def preprocess_review(review):\n",
    "    review = re.sub('[^a-zA-z0-9\\s]','',review.lower())\n",
    "    sequence = tokenizer.texts_to_sequences([review])\n",
    "    feature_vec = pad_sequences(sequence, maxlen=feature_len)\n",
    "    return feature_vec\n",
    "\n",
    "model = load_model('LSTM_v0.1.h5')\n",
    "labels = ['VeryNegative', 'Negative', 'Netural', 'Positive', 'VeryPositive']\n",
    "\n",
    "def label_review(review):\n",
    "    feature_vec=preprocess_review(review)\n",
    "    predict_values = model.predict(feature_vec, batch_size=32, verbose=2, steps=None)\n",
    "    label_index = np.argmax(predict_values)\n",
    "    \n",
    "    \n",
    "#     print('\"', review, '\"', ' IS ', labels[label_index])\n",
    "    \n",
    "    return label_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8527, 3)\n",
      "acc = \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv('../100k-courseras-course-reviews-dataset/reviews_test.csv')\n",
    "data = data[['Review','Label']]\n",
    "data['Predict_Label'] = data['Review'].apply(lambda x: label_review(x))\n",
    "\n",
    "corrects = data[ data['Label'] == data['Predict_Label']]\n",
    "print(corrects.shape)\n",
    "print('acc = ', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
